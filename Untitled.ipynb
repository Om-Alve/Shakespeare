{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d537c5c4-7f70-4e72-9a92-39a7a57f7b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\.conda\\envs\\acehacks\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 9732084\n",
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3001 [00:00<?, ?it/s]C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_19664\\3801842891.py:81: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  out = torch.nn.functional.scaled_dot_product_attention(\n",
      "  0%|                                                                                 | 3/3001 [00:00<11:17,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 : loss = 5.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                              | 65/3001 [00:07<05:44,  8.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits,y)\n\u001b[0;32m    156\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 157\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\acehacks\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\acehacks\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import basic torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import BytePairEncoder,GPT2Config\n",
    "from tqdm import tqdm\n",
    "from minbpe.minbpe import BasicTokenizer\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 64\n",
    "max_iters = 3000\n",
    "eval_interval= 300\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eval_iters = 200\n",
    "n_embed = 512\n",
    "n_heads = 8\n",
    "n_layer = 3\n",
    "dropout = 0.2\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "with open('shakespeare.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab_size = 500\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    return x,y\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in [\"train\",\"test\"]:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             x = get_batch(split)\n",
    "#             logits = model(x.to(device))\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "#     model.train()\n",
    "#     return out\n",
    "\n",
    "\n",
    "# Transformer Class\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, n_embed: int):\n",
    "        super().__init__()\n",
    "        hidden_size = int(n_embed * (4 * (2/3)))  # Corrected calculation\n",
    "        self.w = nn.Linear(n_embed, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(n_embed, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, n_embed, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.w.weight)\n",
    "        nn.init.xavier_uniform_(self.v.weight)\n",
    "        nn.init.xavier_uniform_(self.w2.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.dropout(self.w2(F.silu(self.w(x)) * self.v(x)))\n",
    "        return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size: int, n_embed: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(n_embed, 3 * head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.qkv.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, -1).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=self.dropout.p if training else 0.0, is_causal=True\n",
    "        )\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embed: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size,n_embed, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        out = torch.cat([head(X, training) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, n_embed // n_heads, n_embed, dropout)\n",
    "        self.ffwd = SwiGLU(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.sa_heads(x, training)\n",
    "        x = self.ln2(x)\n",
    "        return x + self.ffwd(x)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config :GPT2Config):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
    "        self.positon_embedding = nn.Embedding(config.block_size, config.n_embed)\n",
    "        self.blocks = nn.ModuleList([Block(config.n_embed, config.n_heads, config.dropout) for _ in range(config.n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
    "        # Weight Tying\n",
    "        self.embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.embedding_table(x)\n",
    "        pos_emb = self.positon_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = BasicTokenizer()\n",
    "    tokenizer.load(\"tokenizer.model\")\n",
    "    data = torch.tensor(tokenizer.encode(text),dtype=torch.long)\n",
    "    split = int(len(data) * 0.9)\n",
    "    train_data = data[:split]\n",
    "    test_data = data[split:]\n",
    "    config = GPT2Config(vocab_size=vocab_size,block_size=block_size,n_embed=n_embed,n_heads=n_heads,n_layers=n_layer,dropout=dropout,lr=learning_rate,t_max=max_iters)\n",
    "    model = TransformerDecoder(config).to(device)\n",
    "    print(\"Total Params:\",sum([p.numel() for p in model.parameters()]))\n",
    "    print(\"Using\",device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=max_iters)\n",
    "\n",
    "    for iter in tqdm(range(max_iters+1)):\n",
    "        x,y = get_batch(\"train\")\n",
    "        logits = model(x)\n",
    "        logits = logits.view(-1,logits.shape[-1])\n",
    "        y = y.view(-1)\n",
    "        loss = F.cross_entropy(logits,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        if iter % eval_interval == 0:\n",
    "            with torch.inference_mode():\n",
    "                model.eval()\n",
    "                x,y = get_batch(\"test\")\n",
    "                logits = model(x)\n",
    "                logits = logits.view(-1,logits.shape[-1])\n",
    "                y = y.view(-1)\n",
    "                loss = F.cross_entropy(logits,y)\n",
    "                tqdm.write(f\"iter {iter} : loss = {loss.item():.4f}\")\n",
    "                model.train()\n",
    "\n",
    "    torch.save(model.state_dict(),\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb636922-d2ba-4a0b-bb30-8fb3596452ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,y = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc010ce-6836-4265-95bd-539f6d802352",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[107,  32, 322,  ..., 343, 386, 311],\n",
       "         [311, 360, 115,  ..., 256, 269, 108],\n",
       "         [424, 111, 355,  ..., 103, 326,  99],\n",
       "         ...,\n",
       "         [384, 109, 364,  ..., 260, 325, 115],\n",
       "         [308, 107, 256,  ..., 422, 276, 415],\n",
       "         [309, 305, 108,  ..., 396, 292, 437]], device='cuda:0'),\n",
       " tensor([[ 32, 322, 308,  ..., 386, 311, 291],\n",
       "         [360, 115, 263,  ..., 269, 108, 284],\n",
       "         [111, 355, 286,  ..., 326,  99, 329],\n",
       "         ...,\n",
       "         [109, 364, 303,  ..., 325, 115, 408],\n",
       "         [107, 256,  97,  ..., 276, 415, 107],\n",
       "         [305, 108, 274,  ..., 292, 437, 414]], device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:50],y[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc497eec-9a8a-4971-91ea-e5aa05261d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 500])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9d694b6-2ea8-4d4b-8334-08af31de1200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3462, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits,y.view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acehacks",
   "language": "python",
   "name": "acehacks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
